# -*- coding: utf-8 -*-
"""MLOD code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lbDfTknC72U_TV0RFzciTsMjwK1t8LRT
"""

#Loading dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold,KFold
from sklearn.metrics import mean_squared_error,mean_absolute_error
from bayes_opt import BayesianOptimization
from sklearn.ensemble import RandomForestRegressor
from lightgbm import LGBMRegressor
import keras
from keras.models import Sequential
from keras.layers import Dense,Dropout,Lambda
from keras import regularizers
from keras.optimizers import Adam,Nadam
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import learning_curve
from sklearn.model_selection import cross_val_score
import warnings,gc,math
warnings.filterwarnings("ignore")

    #Import Data
data = pd.read_excel(r'Data.xlsx')
print(data.head())
print(data.shape)

    #Correlation Plot
corrmat = data.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(9,9))
plt.title("Correlation Plot")
g = sns.heatmap(data[top_corr_features].corr(),annot=True,cmap="RdYlGn") #plot heat map

    #Scaling Data: Standardisation
sc = StandardScaler()
sc_data = pd.DataFrame(sc.fit_transform(data))

    #Splitting the Data
X = sc_data.drop(labels = 'OP', axis = 1)
Y = sc_data[['OP']]

    ## 1. Random Forest (RF)
    
    #Defining RF for BO
Bayes_sc_data = sc_data.sample(frac = 0.2, random_state = 24) #BO carried out on 20% of the available data
Bayes_X = Bayes_sc_data.drop(labels = 'OP', axis = 1)
Bayes_Y = Bayes_sc_data[['OP']]
Bayes_X_train, Bayes_X_val, Bayes_Y_train, Bayes_Y_val = train_test_split(Bayes_X, Bayes_Y, test_size = 0.2, random_state = 42)

def RF_bayesian(
    n_estimators,
    max_features,
    max_depth,
    min_samples_leaf,
    max_leaf_nodes):
    

    rf_bo = RandomForestRegressor(n_estimators=int(n_estimators),
                                  max_features=int(max_features),
                                  max_depth=int(max_depth),
                                  min_samples_leaf=int(min_samples_leaf),
                                  max_leaf_nodes=int(max_leaf_nodes),
                                  criterion='mse',
                                  random_state=42)
    
    rf_bo.fit(Bayes_X_train, Bayes_Y_train)
   
    Bayes_Y_val_pred  = rf_bo.predict(Bayes_X_val)  
    
    score = mean_squared_error(Bayes_Y_val, Bayes_Y_val_pred)

    return (-1*score)

    #Bounded region of RF parameter space
bounds_rf = {
    'n_estimators': (50, 500),
    'max_features': (2, 7),
    'max_depth':(1, 150),
    'min_samples_leaf': (2, 10),
    'max_leaf_nodes': (2, 50)}

    #Bayesian Optimization
RF_BO = BayesianOptimization(RF_bayesian, bounds_rf, random_state = 42)
print(RF_BO.space.keys)
init_points = 10
n_iter = 40
print('-' * 130)
RF_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)
print(RF_BO.max['target'])
print(RF_BO.max['params'])

    #LGBM Model Development
RF_Reg = RandomForestRegressor(n_estimators = int(RF_BO.max['params']['n_estimators']),
                                   max_features = int(RF_BO.max['params']['max_features']),
                                   max_depth = int(RF_BO.max['params']['max_depth']),
                                   min_samples_leaf = int(RF_BO.max['params']['min_samples_leaf']),
                                   max_leaf_nodes = int(RF_BO.max['params']['max_leaf_nodes']),
                                   random_state = 0)
RF_Reg

    #Cross-validated Results
KF = KFold(n_splits= 10, shuffle = True, random_state= 24)

SMSE_scores = (-1 * cross_val_score(RF_Reg, X, Y, cv= KF, scoring = 'neg_mean_squared_error'))
SMAE_scores = (-1 * cross_val_score(RF_Reg, X, Y, cv= KF, scoring = 'neg_mean_absolute_error'))

SRMSE_scores = np.sqrt(SMSE_scores)

print('-' * 130)
print("     Scaled Results of the Cross-Validated RF Model with Optimised Hyperparameters")

Mean_CV_SRMSE = np.round(np.mean(SRMSE_scores),4)
SD_CV_SRMSE = np.round(np.std(SRMSE_scores, ddof = 1),4)
CoV_SRMSE = np.round(((SD_CV_SRMSE/Mean_CV_SRMSE)*100),2)

print("Mean of the SRMSE across 10 folds is: ", Mean_CV_SRMSE)
print("Standard Deviation of SRMSE is: ", SD_CV_SRMSE)
print("Coefficient of Variation of SRMSE is: ", CoV_SRMSE)

Mean_CV_SMAE = np.round(np.mean(SMAE_scores),4)
SD_CV_SMAE = np.round(np.std(SMAE_scores, ddof = 1),4)
CoV_SMAE = np.round(((SD_CV_SMAE/Mean_CV_SMAE)*100),2)

print("Mean of the SMAE across 10 folds is: ", Mean_CV_SMAE)
print("Standard Deviation of SMAE is: ", SD_CV_SMAE)
print("Coefficient of Variation of SMAE is: ", CoV_SMAE)

print('-' * 130)

    #Plotting Learning Curves
#for SRMSE
train_sizes = [1, 50, 100, 250, 500, 750, 1000, 1250, 1500, 1683]
train_sizes, SMSE_train_scores, SMSE_validation_scores = learning_curve(RF_Reg, X, Y, train_sizes = train_sizes, cv = KF, scoring = 'neg_mean_squared_error')
SRMSE_train_scores = np.sqrt(-1*SMSE_train_scores)
SRMSE_validation_scores = np.sqrt(-1*SMSE_validation_scores)
SRMSE_train_scores_mean = SRMSE_train_scores.mean(axis = 1)
SRMSE_validation_scores_mean = SRMSE_validation_scores.mean(axis = 1)
print('Mean training scores\n\n', pd.Series(SRMSE_train_scores_mean, index = train_sizes))
print('\n', '-' * 20) # separator
print('\nMean validation scores\n\n',pd.Series(SRMSE_validation_scores_mean, index = train_sizes))

plt.style.use('seaborn')
plt.plot(train_sizes, SRMSE_train_scores_mean, label = 'Training error')
plt.plot(train_sizes, SRMSE_validation_scores_mean, label = 'Validation error')
plt.ylabel('SRMSE', fontsize = 14)
plt.xlabel('Training set size', fontsize = 14)
plt.title('Learning curves for Scaled RF model', fontsize = 18, y = 1.03)
plt.legend()

#for SMAE
train_sizes, SMAE_train_scores, SMAE_validation_scores = learning_curve(RF_Reg, X, Y, train_sizes = train_sizes, cv = KF, scoring = 'neg_mean_absolute_error')
SMAE_train_scores_mean = -1*SMAE_train_scores.mean(axis = 1)
SMAE_validation_scores_mean = -1*SMAE_validation_scores.mean(axis = 1)
print('Mean training scores\n\n', pd.Series(SMAE_train_scores_mean, index = train_sizes))
print('\n', '-' * 20) # separator
print('\nMean validation scores\n\n',pd.Series(SMAE_validation_scores_mean, index = train_sizes))

plt.style.use('seaborn')
plt.plot(train_sizes, SMAE_train_scores_mean, label = 'Training error')
plt.plot(train_sizes, SMAE_validation_scores_mean, label = 'Validation error')
plt.ylabel('SMAE', fontsize = 14)
plt.xlabel('Training set size', fontsize = 14)
plt.title('Learning curves for a Scaled RF model', fontsize = 18, y = 1.03)
plt.legend()

    ## 2. LightGBM (LGB)
    
    #Defining LGB for BO
Bayes_sc_data = sc_data.sample(frac = 0.2, random_state = 24) #BO carried out on 20% of the available data
Bayes_X = Bayes_sc_data.drop(labels = 'OP', axis = 1)
Bayes_Y = Bayes_sc_data[['OP']]
Bayes_X_train, Bayes_X_val, Bayes_Y_train, Bayes_Y_val = train_test_split(Bayes_X, Bayes_Y, test_size = 0.2, random_state = 42)

def LGB_bayesian(
    learning_rate,
    num_leaves, 
    bagging_fraction,
    feature_fraction,
    min_data_in_leaf,
    max_depth):
    lgb_bo = LGBMRegressor(boosting_type='gbdt',
                           random_state = 42,
                           num_leaves = int(num_leaves),
                           max_depth = int(max_depth),
                           learning_rate = learning_rate,
                           min_data_in_leaf = int(min_data_in_leaf),
                           bagging_fraction = bagging_fraction,
                           feature_fraction = feature_fraction)
    
    lgb_bo.fit(Bayes_X_train, Bayes_Y_train)
   
    Bayes_Y_val_pred  = lgb_bo.predict(Bayes_X_val)  
    
    score = mean_squared_error(Bayes_Y_val, Bayes_Y_val_pred)

    return (-1*score)

    #Bounded region of LGBM parameter space
bounds_LGB = {
    'num_leaves': (5, 500), 
    'min_data_in_leaf': (10, 200),
    'bagging_fraction' : (0.01, 1),
    'feature_fraction' : (0.01, 1),
    'learning_rate': (0.001, .5),
    'max_depth':(2,50)}

    #Bayesian Optimization
LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state = 42)
print(LGB_BO.space.keys)
init_points = 10
n_iter = 50
print('-' * 130)

LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)
    
print(LGB_BO.max['target'])
print(LGB_BO.max['params'])

    #LGB Model Development
LGB_Reg = LGBMRegressor(boosting_type='gbdt',
                             random_state = 0,
                             num_leaves = int(LGB_BO.max['params']['num_leaves']),
                             max_depth = int(LGB_BO.max['params']['max_depth']),
                             learning_rate = LGB_BO.max['params']['learning_rate'],
                             min_data_in_leaf = int(LGB_BO.max['params']['min_data_in_leaf']),
                             bagging_fraction = LGB_BO.max['params']['bagging_fraction'],
                             feature_fraction = LGB_BO.max['params']['feature_fraction'],
                             objective = 'rmse')
LGB_Reg

    #Cross-validated Results
KF = KFold(n_splits= 10, shuffle = True, random_state= 24)

SMSE_scores = (-1 * cross_val_score(LGB_Reg, X, Y, cv= KF, scoring = 'neg_mean_squared_error'))
SMAE_scores = (-1 * cross_val_score(LGB_Reg, X, Y, cv= KF, scoring = 'neg_mean_absolute_error'))

SRMSE_scores = np.sqrt(SMSE_scores)

print('-' * 130)
print("     Scaled Results of the Cross-Validated LightGBM Model with Optimised Hyperparameters")

Mean_CV_SRMSE = np.round(np.mean(SRMSE_scores),4)
SD_CV_SRMSE = np.round(np.std(SRMSE_scores, ddof = 1),4)
CoV_SRMSE = np.round(((SD_CV_SRMSE/Mean_CV_SRMSE)*100),2)

print("Mean of the SRMSE across 10 folds is: ", Mean_CV_SRMSE)
print("Standard Deviation of SRMSE is: ", SD_CV_SRMSE)
print("Coefficient of Variation of SRMSE is: ", CoV_SRMSE)

Mean_CV_SMAE = np.round(np.mean(SMAE_scores),4)
SD_CV_SMAE = np.round(np.std(SMAE_scores, ddof = 1),4)
CoV_SMAE = np.round(((SD_CV_SMAE/Mean_CV_SMAE)*100),2)

print("Mean of the SMAE across 10 folds is: ", Mean_CV_SMAE)
print("Standard Deviation of SMAE is: ", SD_CV_SMAE)
print("Coefficient of Variation of SMAE is: ", CoV_SMAE)

print('-' * 130)

    #Plotting Learning Curves
#for SRMSE
train_sizes = [1, 50, 100, 250, 500, 750, 1000, 1250, 1500, 1683]
train_sizes, SMSE_train_scores, SMSE_validation_scores = learning_curve(LGB_Reg, X, Y, train_sizes = train_sizes, cv = KF, scoring = 'neg_mean_squared_error')
SRMSE_train_scores = np.sqrt(-1*SMSE_train_scores)
SRMSE_validation_scores = np.sqrt(-1*SMSE_validation_scores)
SRMSE_train_scores_mean = SRMSE_train_scores.mean(axis = 1)
SRMSE_validation_scores_mean = SRMSE_validation_scores.mean(axis = 1)
print('Mean training scores\n\n', pd.Series(SRMSE_train_scores_mean, index = train_sizes))
print('\n', '-' * 20) # separator
print('\nMean validation scores\n\n',pd.Series(SRMSE_validation_scores_mean, index = train_sizes))

plt.style.use('seaborn')
plt.plot(train_sizes, SRMSE_train_scores_mean, label = 'Training error')
plt.plot(train_sizes, SRMSE_validation_scores_mean, label = 'Validation error')
plt.ylabel('SRMSE', fontsize = 14)
plt.xlabel('Training set size', fontsize = 14)
plt.title('Learning curves for Scaled LightGBM model', fontsize = 18, y = 1.03)
plt.legend()

#for SMAE
train_sizes, SMAE_train_scores, SMAE_validation_scores = learning_curve(LGB_Reg, X, Y, train_sizes = train_sizes, cv = KF, scoring = 'neg_mean_absolute_error')
SMAE_train_scores_mean = -1*SMAE_train_scores.mean(axis = 1)
SMAE_validation_scores_mean = -1*SMAE_validation_scores.mean(axis = 1) 
print('Mean training scores\n\n', pd.Series(SMAE_train_scores_mean, index = train_sizes))
print('\n', '-' * 20) # separator
print('\nMean validation scores\n\n',pd.Series(SMAE_validation_scores_mean, index = train_sizes))

plt.style.use('seaborn')
plt.plot(train_sizes, SMAE_train_scores_mean, label = 'Training error')
plt.plot(train_sizes, SMAE_validation_scores_mean, label = 'Validation error')
plt.ylabel('SMAE', fontsize = 14)
plt.xlabel('Training set size', fontsize = 14)
plt.title('Learning curves for a Scaled LightGBM model', fontsize = 18, y = 1.03)
plt.legend()

    ## 3. Artificial Neural Network (ANN)
    
    #Defining ANN for BO
Bayes_sc_data = sc_data.sample(frac = 0.2, random_state = 24) #BO carried out on 20% of the available data
Bayes_X = Bayes_sc_data.drop(labels = 'OP', axis = 1)
Bayes_Y = Bayes_sc_data[['OP']]
Bayes_X_train, Bayes_X_val, Bayes_Y_train, Bayes_Y_val = train_test_split(Bayes_X, Bayes_Y, test_size = 0.2, random_state = 42)

def ANN_bayesian(
    learning_rate,
    no_of_neurons1,
    no_of_neurons2,
    no_of_neurons3,
    no_of_neurons4,
    no_layers,
    dropout1,
    dropout2,
    dropout3,
    dropout4,
    batch_size,
    epochs
    ):
    
    #dp1 = dropout1
    #dp2 =dropout2
    
    dps = [dropout1,dropout2,dropout3,dropout4]
    ns=[int(no_of_neurons1),int(no_of_neurons2),int(no_of_neurons3),int(no_of_neurons4)]
    n_layer = int(no_layers)
    lr = learning_rate
    bat_sz = int(batch_size)
    ephs = int(epochs)
    
    ann_bo = Sequential()
    ann_bo.add(Dense(output_dim =ns[0], init = 'normal', activation = 'tanh', input_dim = 8))
    ann_bo.add(Dropout(rate=dps[0]))
        
    for i in range(1,n_layer):
        ann_bo.add(Dense(output_dim =ns[i], init = 'normal', activation = 'tanh'))
        ann_bo.add(Dropout(rate=dps[i]))
    
    ann_bo.add(Dense(output_dim = 1,activation='linear'))

    ann_bo.compile(optimizer =Nadam(lr=lr), loss = 'mean_squared_error', metrics = ['mean_squared_error'])

    ann_bo.fit(Bayes_X_train, Bayes_Y_train, verbose=0, batch_size=bat_sz, epochs =ephs, shuffle = True)
    
    Bayes_Y_val_pred  = ann_bo.predict(Bayes_X_val)  
    
    score = mean_squared_error(Bayes_Y_val, Bayes_Y_val_pred)

    return (-1*score)

bound_spaces_ann = {'learning_rate': (0.001, 0.5),
                    'no_of_neurons1':(1,12),
                    'no_of_neurons2':(1,12),
                    'no_of_neurons3':(1,12),
                    'no_of_neurons4':(1,12),
                    'dropout1':(0,0.8),
                    'dropout2':(0,0.8),
                    'dropout3':(0,0.8),
                    'dropout4':(0,0.8),
                    'batch_size':(8,128),
                    'no_layers':(1,4),
                    'epochs' : (10, 250)}

    #Bayesian Optimization
ANN_BO = BayesianOptimization(ANN_bayesian, bound_spaces_ann, random_state = 42)
print(ANN_BO.space.keys)
init_points = 10
n_iter = 50
print('-' * 130)

ANN_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)
    
print(ANN_BO.max['target'])
print(ANN_BO.max['params'])

dp1=ANN_BO.max['params']['dropout1']
dp2 =ANN_BO.max['params']['dropout2']
dp3 =ANN_BO.max['params']['dropout3']
dp4 =ANN_BO.max['params']['dropout4']
lr = ANN_BO.max['params']['learning_rate']
hidden1= int(ANN_BO.max['params']['no_of_neurons1'])
hidden2= int(ANN_BO.max['params']['no_of_neurons2'])
hidden3= int(ANN_BO.max['params']['no_of_neurons3'])
hidden4= int(ANN_BO.max['params']['no_of_neurons4'])
bat_sz = int(ANN_BO.max['params']['batch_size'])
ephs = int(ANN_BO.max['params']['epochs'])
no_layers = int(ANN_BO.max['params']['no_layers'])

dps = [dp1,dp2,dp3,dp4 ]
ns=[int(hidden1),int(hidden2),int(hidden3),int(hidden4)]
n_layer = int(no_layers)

def create_ANN():
    ANN_Reg = Sequential()
    ANN_Reg.add(Dense(output_dim =ns[0], init = 'normal', activation = 'tanh', input_dim = 8))
    ANN_Reg.add(Dropout(rate=dps[0]))
        
    for i in range(1,n_layer):
        ANN_Reg.add(Dense(output_dim =ns[i], init = 'normal', activation = 'tanh'))
        ANN_Reg.add(Dropout(rate=dps[i]))
    
    ANN_Reg.add(Dense(output_dim = 1,activation='linear'))
    ANN_Reg.compile(optimizer =Nadam(lr=lr), loss = 'mean_squared_error', metrics = ['mean_squared_error'])
    return ANN_Reg
ANN_Reg = KerasRegressor(build_fn=create_ANN, epochs=ephs, batch_size=bat_sz, verbose=0)

    #Cross-validated Results
KF = KFold(n_splits= 10, shuffle = True, random_state= 24)

SMSE_scores = (-1 * cross_val_score(ANN_Reg, X, Y, cv= KF, scoring = 'neg_mean_squared_error'))
SMAE_scores = (-1 * cross_val_score(ANN_Reg, X, Y, cv= KF, scoring = 'neg_mean_absolute_error'))

SRMSE_scores = np.sqrt(SMSE_scores)

print('-' * 130)
print("     Scaled Results of the Cross-Validated ANN Model with Optimised Hyperparameters")

Mean_CV_SRMSE = np.round(np.mean(SRMSE_scores),4)
SD_CV_SRMSE = np.round(np.std(SRMSE_scores, ddof = 1),4)
CoV_SRMSE = np.round(((SD_CV_SRMSE/Mean_CV_SRMSE)*100),2)

print("Mean of the SRMSE across 10 folds is: ", Mean_CV_SRMSE)
print("Standard Deviation of SRMSE is: ", SD_CV_SRMSE)
print("Coefficient of Variation of SRMSE is: ", CoV_SRMSE)

Mean_CV_SMAE = np.round(np.mean(SMAE_scores),4)
SD_CV_SMAE = np.round(np.std(SMAE_scores, ddof = 1),4)
CoV_SMAE = np.round(((SD_CV_SMAE/Mean_CV_SMAE)*100),2)

print("Mean of the SMAE across 10 folds is: ", Mean_CV_SMAE)
print("Standard Deviation of SMAE is: ", SD_CV_SMAE)
print("Coefficient of Variation of SMAE is: ", CoV_SMAE)

print('-' * 130)

    #Plotting Learning Curves
#for SRMSE
train_sizes = [1, 50, 100, 250, 500, 750, 1000, 1250, 1500, 1683]
train_sizes, SMSE_train_scores, SMSE_validation_scores = learning_curve(ANN_Reg, X, Y, train_sizes = train_sizes, cv = KF, scoring = 'neg_mean_squared_error')
SRMSE_train_scores = np.sqrt(-1*SMSE_train_scores)
SRMSE_validation_scores = np.sqrt(-1*SMSE_validation_scores)
SRMSE_train_scores_mean = SRMSE_train_scores.mean(axis = 1)
SRMSE_validation_scores_mean = SRMSE_validation_scores.mean(axis = 1)
print('Mean training scores\n\n', pd.Series(SRMSE_train_scores_mean, index = train_sizes))
print('\n', '-' * 20) # separator
print('\nMean validation scores\n\n',pd.Series(SRMSE_validation_scores_mean, index = train_sizes))

plt.style.use('seaborn')
plt.plot(train_sizes, SRMSE_train_scores_mean, label = 'Training error')
plt.plot(train_sizes, SRMSE_validation_scores_mean, label = 'Validation error')
plt.ylabel('SRMSE', fontsize = 14)
plt.xlabel('Training set size', fontsize = 14)
plt.title('Learning curves for Scaled ANN model', fontsize = 18, y = 1.03)
plt.legend()

#for SMAE
train_sizes, SMAE_train_scores, SMAE_validation_scores = learning_curve(ANN_Reg, X, Y, train_sizes = train_sizes, cv = KF, scoring = 'neg_mean_absolute_error')
SMAE_train_scores_mean = -1*SMAE_train_scores.mean(axis = 1)
SMAE_validation_scores_mean = -1*SMAE_validation_scores.mean(axis = 1)
print('Mean training scores\n\n', pd.Series(SMAE_train_scores_mean, index = train_sizes))
print('\n', '-' * 20) # separator
print('\nMean validation scores\n\n',pd.Series(SMAE_validation_scores_mean, index = train_sizes))

plt.style.use('seaborn')
plt.plot(train_sizes, SMAE_train_scores_mean, label = 'Training error')
plt.plot(train_sizes, SMAE_validation_scores_mean, label = 'Validation error')
plt.ylabel('SMAE', fontsize = 14)
plt.xlabel('Training set size', fontsize = 14)
plt.title('Learning curves for a Scaled ANN model', fontsize = 18, y = 1.03)
plt.legend()